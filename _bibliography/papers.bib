---
---

@string{aps = {American Physical Society,}}

@article{lu2011join,
  title={Join-idle-queue: A novel load balancing algorithm for dynamically scalable web services},
  author={Lu, Yi and Xie, Qiaomin and Kliot, Gabriel and Geller, Alan and Larus, James R and Greenberg, Albert},
  journal={Performance Evaluation},
  volume={68},
  number={11},
  pages={1056--1071},
  year={2011},
  publisher={North-Holland},
  url={https://dl.acm.org/doi/10.1016/j.peva.2011.07.015}
}

@inproceedings{xie2012degree,
  title={Degree-guided map-reduce task assignment with data locality constraint},
  author={Xie, Qiaomin and Lu, Yi},
  booktitle={2012 IEEE International Symposium on Information Theory Proceedings},
  pages={985--989},
  year={2012},
  organization={IEEE},
  url={https://ieeexplore.ieee.org/document/6284711}
}

@inproceedings{xie2015power,
  title={Power of d Choices for Large-Scale Bin Packing: A Loss Model},
  author={Xie, Qiaomin and Dong, Xiaobo and Lu, Yi and Srikant, R},
  booktitle={ACM Sigmetrics},
  year={2015},
  url={https://dl.acm.org/doi/10.1145/2745844.2745849}
}

@inproceedings{xie2015priority,
  title={Priority algorithm for near-data scheduling: Throughput and heavy-traffic optimality},
  author={Xie, Qiaomin and Lu, Yi},
  booktitle={2015 IEEE Conference on Computer Communications (INFOCOM)},
  pages={963--972},
  year={2015},
  organization={IEEE},
  url={https://ieeexplore.ieee.org/document/7218468},
}

@inproceedings{xie2016scheduling,
  title={Scheduling with Multi-level Data Locality: Throughput and Heavy-Traffic Optimality},
  author={Xie, Qiaomin and Yekkehkhany, Ali and Lu, Yi},
  booktitle={2016 IEEE Conference on Computer Communications (INFOCOM)},
  year={2016},
  url={https://ieeexplore.ieee.org/document/7524416},
}

@article{xie2016pandas,
  title={Pandas: robust locality-aware scheduling with stochastic delay optimality},
  author={Xie, Qiaomin and Pundir, Mayank and Lu, Yi and Abad, Cristina L and Campbell, Roy H},
  journal={IEEE/ACM Transactions on Networking},
  volume={25},
  number={2},
  pages={662--675},
  year={2016},
  publisher={IEEE},
  url={https://ieeexplore.ieee.org/document/7582519},
}

@inproceedings{gupta2017stochastic,
  title={Stochastic online scheduling on unrelated machines},
  author={Gupta, Varun and Moseley, Benjamin and Uetz, Marc and Xie, Qiaomin},
  booktitle={International Conference on Integer Programming and Combinatorial Optimization},
  pages={228--240},
  year={2017},
  organization={Springer, Cham},
  url={https://link.springer.com/chapter/10.1007/978-3-319-59250-3_19}
}


@article{shah2017centralized,
  title={Centralized Congestion Control and Scheduling in a Datacenter},
  author={Shah, Devavrat and Xie, Qiaomin},
  journal={arXiv preprint arXiv:1710.02548},
  year={2017},
  arxiv={https://arxiv.org/abs/1710.02548}
}

@inproceedings{shah2018q,
  title={Q-learning with nearest neighbors},
  author={Shah, Devavrat and Xie, Qiaomin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018},
  arxiv={https://arxiv.org/abs/1802.03900},
  url={https://proceedings.neurips.cc/paper_files/paper/2018/hash/309fee4e541e51de2e41f21bebb342aa-Abstract.html}
}

@inproceedings{liu2019reinforcement,
  title={Reinforcement learning for optimal control of queueing systems},
  author={Liu, Bai and Xie, Qiaomin and Modiano, Eytan},
  booktitle={2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={663--670},
  year={2019},
  organization={IEEE},
  url={https://ieeexplore.ieee.org/document/8919665}
}

@inproceedings{fei2020dynamic,
  title={Dynamic Regret of Policy Optimization in Non-Stationary Environments},
  author={Fei, Yingjie and Yang, Zhuoran and Wang, Zhaoran and Xie, Qiaomin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020},
  arxiv={https://arxiv.org/abs/2007.00148},
  url={https://proceedings.neurips.cc/paper_files/paper/2020/hash/4b0091f82f50ff7095647fe893580d60-Abstract.html}
}

@inproceedings{mao2020poly,
  title={POLY-HOOT: Monte-Carlo Planning in Continuous Space MDPs with Non-Asymptotic Analysis},
  author={Mao, Weichao and Zhang, Kaiqing and Xie, Qiaomin and Basar, Tamer},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020},
  arxiv={https://arxiv.org/abs/2006.04672},
  url={https://proceedings.neurips.cc/paper_files/paper/2020/hash/30de24287a6d8f07b37c716ad51623a7-Abstract.html},
}

@inproceedings{fei2020risk,
  title={Risk-Sensitive Reinforcement Learning: Near-Optimal Risk-Sample Tradeoff in Regret},
  author={Fei, Yingjie and Yang, Zhuoran and Chen, Yudong and Wang, Zhaoran and Xie, Qiaomin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020},
  arxiv={https://arxiv.org/abs/2006.13827},
  url={https://proceedings.neurips.cc/paper_files/paper/2020/hash/fdc42b6b0ee16a2f866281508ef56730-Abstract.html}
}

@inproceedings{shah2020stable,
  title={Stable Reinforcement Learning with Unbounded State Space},
  author={Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
  booktitle={Learning for Dynamics and Control (L4DC)},
  pages={581},
  year={2020},
  arxiv={https://arxiv.org/abs/2006.04353}
}

@inproceedings{shah2020reinforcement,
  title={On Reinforcement Learning for Turn-based Zero-sum Markov Games},
  author={Shah, Devavrat and Somani, Varun and Xie, Qiaomin and Xu, Zhi},
  booktitle={Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference},
  pages={pp--139},
  year={2020},
  arxiv={https://arxiv.org/abs/2002.10620}
}


@inproceedings{xie2020learning,
  title={Learning zero-sum simultaneous-move Markov games using function approximation and correlated equilibrium},
  author={Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={Conference on Learning Theory},
  pages={3674--3682},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v125/xie20a.html}
}

@inproceedings{shah2020non,
  title={Non-asymptotic analysis of Monte Carlo tree search},
  author={Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
  booktitle={ACM Sigmetrics},
  pages={31--32},
  year={2020},
  url={https://dl.acm.org/doi/abs/10.1145/3393691.3394202}
}

@article{gupta2020greed,
  title={Greed worksâ€”online algorithms for unrelated machine stochastic scheduling},
  author={Gupta, Varun and Moseley, Benjamin and Uetz, Marc and Xie, Qiaomin},
  journal={Mathematics of operations research},
  volume={45},
  number={2},
  pages={497--516},
  year={2020},
  publisher={INFORMS},
  arixv={https://arxiv.org/abs/1703.01634},
  url={https://pubsonline.informs.org/doi/abs/10.1287/moor.2019.0999},
}

@inproceedings{xie2021learning,
  title={Learning While Playing in Mean-Field Games: Convergence and Optimality},
  author={Xie, Qiaomin and Yang, Zhuoran and Wang, Zhaoran and Minca, Andreea},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={11436--11447},
  year={2021},
  organization={PMLR},
  html={https://proceedings.mlr.press/v139/xie21g.html}
}


@inproceedings{wang2021zero,
  title={Zero queueing for multi-server jobs},
  author={Wang, Weina and Xie, Qiaomin and Harchol-Balter, Mor},
  booktitle={ACM Sigmetrics},
  pages={13--14},
  year={2021},
  arxiv={https://arxiv.org/abs/2011.10521},
  html={https://dl.acm.org/doi/10.1145/3447385}
}

@article{archer2022orsuite,
author = {Archer, Christopher and Banerjee, Siddhartha and Cortez, Mayleen and Rucker, Carrie and Sinclair, Sean R. and Solberg, Max and Xie, Qiaomin and Lee Yu, Christina},
title = {ORSuite: Benchmarking Suite for Sequential Operations Models},
year = {2022},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/3512798.3512819},
doi = {10.1145/3512798.3512819},
abstract = {Reinforcement learning (RL) has received widespread attention across multiple communities, but the experiments have focused primarily on large-scale game playing and robotics tasks. In this paper we introduce ORSuite, an open-source library containing environments, algorithms, and instrumentation for operational problems. Our package is designed to motivate researchers in the reinforcement learning community to develop and evaluate algorithms on operational tasks, and to consider the true multi-objective nature of these problems by considering metrics beyond cumulative reward.},
journal = {SIGMETRICS Performance Evaluation Review},
month = {jan},
pages = {57--61},
numpages = {5}
}

@article{shah2022non_OR,
  title={Nonasymptotic Analysis of {Monte Carlo} Tree Search},
  author={Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
  journal={Operations Research},
  volume={70},
  number={6},
  pages={3234--3260},
  year={2022},
  publisher={INFORMS},
  doi={10.1287/opre.2021.2239},
  html={https://pubsonline.informs.org/doi/abs/10.1287/opre.2021.2239?journalCode=opre}
}

@article{Liu2022rlqn,
author = {Liu, Bai and Xie, Qiaomin and Modiano, Eytan},
title = {RL-QN: A Reinforcement Learning Framework for Optimal Control of Queueing Systems},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2376-3639},
url = {https://doi.org/10.1145/3529375},
doi = {10.1145/3529375},
abstract = {With the rapid advance of information technology, network systems have become increasingly complex and hence the underlying system dynamics are often unknown or difficult to characterize. Finding a good network control policy is of significant importance to achieve desirable network performance (e.g., high throughput or low delay). In this work, we consider using model-based reinforcement learning (RL) to learn the optimal control policy for queueing networks so that the average job delay (or equivalently the average queue backlog) is minimized. Traditional approaches in RL, however, cannot handle the unbounded state spaces of the network control problem. To overcome this difficulty, we propose a new algorithm, called RL for Queueing Networks (RL-QN), which applies model-based RL methods over a finite subset of the state space while applying a known stabilizing policy for the rest of the states. We establish that the average queue backlog under RL-QN with an appropriately constructed subset can be arbitrarily close to the optimal result. We evaluate RL-QN in dynamic server allocation, routing, and switching problems. Simulation results show that RL-QN minimizes the average queue backlog effectively.},
journal = {ACM Transactions on Modeling and Performance Evaluation of Computing Systems},
month = {aug},
articleno = {2},
numpages = {35},
keywords = {Queueing networks, reinforcement learning}
}

@article{xie2023cce_mor,
author = {Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
title = {Learning Zero-Sum Simultaneous-Move Markov Games Using Function Approximation and Correlated Equilibrium},
year = {2023},
issue_date = {February 2023},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {48},
number = {1},
issn = {0364-765X},
url = {https://doi.org/10.1287/moor.2022.1268},
doi = {10.1287/moor.2022.1268},
abstract = {We develop provably efficient reinforcement learning algorithms for two-player zero-sum finite-horizon Markov games with simultaneous moves. To incorporate function approximation, we consider a family of Markov games where the reward function and transition kernel possess a linear structure. Both the offline and online settings of the problems are considered. In the offline setting, we control both players and aim to find the Nash equilibrium by minimizing the duality gap. In the online setting, we control a single player playing against an arbitrary opponent and aim to minimize the regret. For both settings, we propose an optimistic variant of the least-squares minimax value iteration algorithm. We show that our algorithm is computationally efficient and provably achieves an O(d3H3T) upper bound on the duality gap and regret, where d is the linear dimension, H the horizon and T the total number of timesteps. Our results do not require additional assumptions on the sampling model. Our setting requires overcoming several new challenges that are absent in Markov decision processes or turn-based Markov games. In particular, to achieve optimism with simultaneous moves, we construct both upper and lower confidence bounds of the value function, and then compute the optimistic policy by solving a general-sum matrix game with these bounds as the payoff matrices. As finding the Nash equilibrium of a general-sum game is computationally hard, our algorithm instead solves for a coarse correlated equilibrium (CCE), which can be obtained efficiently. To our best knowledge, such a CCE-based scheme for optimism has not appeared in the literature and might be of interest in its own right.},
journal = {Mathematics of Operations Research},
month = {feb},
pages = {433--462},
numpages = {30},
keywords = {Markov games, function approximation, correlated equilibrium, secondary: 91A15, Primary: 68T05, reinforcement learning},
arxiv={https://arxiv.org/abs/2002.07066}
}

@inproceedings{hong2022maximizing,
  title={Near-Optimal Stochastic Bin-Packing in Large Service Systems with Time-Varying Item Sizes},
  author={Hong, Yige and Xie, Qiaomin and Wang, Weina},
  %journal={arXiv preprint arXiv:2209.04123},
  arxiv={https://arxiv.org/abs/2209.04123},
  booktitle={ACM Sigmetrics},
  year={2024},
}

@inproceedings{wu2023reward,
  title={Reward Poisoning Attacks on Offline Multi-Agent Reinforcement Learning},
  author={Wu, Young and McMahan, Jermey and Zhu, Xiaojin and Xie, Qiaomin},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2023},
  arxiv={https://arxiv.org/abs/2206.01888}
}


@inproceedings{huo2023bias,
  title={Bias and Extrapolation in Markovian Linear Stochastic Approximation with Constant Stepsizes},
  author={Huo, Dongyan and Chen, Yudong and Xie, Qiaomin},
  booktitle={ACM Sigmetrics},
  year={2023},
  arxiv={https://arxiv.org/abs/2210.00953}
}

@inproceedings{qin2023distributed,
  title={Distributed Threshold-based Offloading for Heterogeneous Mobile Edge Computing},
  author={Qin, Xudong and Xie, Qiaomin and Li, Bin},
  booktitle={International Conference on Distributed Computing Systems (ICDCS)},
  year={2023},
  abstract={In this paper, we consider a large-scale heterogeneous mobile edge computing system, where each device's
mean computing task arrival rate, mean service rate, mean
energy consumption, and mean offloading latency are drawn
from different bounded continuous probability distributions to
reflect the diverse compute-intensive applications, mobile devices
with different computing capabilities and battery efficiencies,
and different types of wireless access networks (e.g., 4G/5G
cellular networks, WiFi). We consider a class of distributed
threshold-based randomized offloading policies and develop a
threshold update algorithm based on its computational load,
average offloading latency, average energy consumption, and
edge server processing time, depending on the server utilization.
We show that there always exists a unique Mean-Field Nash
Equilibrium (MFNE) in the large-system limit when the task
processing times of mobile devices follow an exponential distribution. This is achieved by carefully partitioning the space of
mean arrival rates to account for the discrete structure of each
device's optimal threshold. Moreover, we show that our proposed
threshold update algorithm converges to the MFNE. Finally, we
perform simulations to corroborate our theoretical results and
demonstrate that our proposed algorithm still performs well in
more general setups based on the collected real-world data and
outperforms the well-known probabilistic offloading policy},
}

@inproceedings{zhang2023average,
  title={Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes},
  author={Zhang, Zihan and Xie, Qiaomin},
  booktitle={Conference on Learning Theory (COLT)},
  year={2023},
  arxiv={https://arxiv.org/abs/2306.16394},
  abstract={We develop several provably efficient model-free reinforcement learning (RL) algorithms for  infinite-horizon average-reward Markov Decision Processes (MDPs). We consider both online setting and the setting with access to a simulator. In the online setting, we propose model-free RL algorithms based on reference-advantage decomposition. Our algorithm achieves $O(S^5A^2{sp}(h^*)\sqrt{T})$ regret after $T$ steps, where $S\times A$ is the size of state-action space, and 
   ${sp}(h^*)$ the span of the optimal bias function. Our results are the first to achieve optimal dependence in $T$ for weakly communicating MDPs. In the simulator setting, we propose a model-free RL algorithm that finds an $\epsilon$-optimal policy using $O (\frac{SA{sp}^2(h^*)}{\epsilon^2}+\frac{S^2A{sp}(h^*)}{\epsilon})$ samples, whereas the minimax lower bound is $\Omega(\frac{SA{sp}(h^*)}{\epsilon^2})$. 
   Our results are based on two new techniques that are unique in the average-reward setting: 1) better discounted approximation by value-difference estimation; 2) efficient construction of confidence region for the optimal bias function with space complexity $O(SA)$.},
}

@article{mukherjee2023speed,
  title={SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits},
  author={Mukherjee, Subhojyoti and Xie, Qiaomin and Hanna, Josiah and Nowak, Robert},
  journal={arXiv preprint arXiv:2301.12357},
  year={2023},
  arxiv={https://arxiv.org/abs/2301.12357}
}

@inproceedings{mukherjee2023multi_task,
  title={Multi-task Representation Learning for Pure Exploration in Bilinear Bandits},
  author={Mukherjee, Subhojyoti and Xie, Qiaomin and Hanna, Josiah and Nowak, Robert},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023}
}

@inproceedings{hong2023restless,
      title={Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption}, 
      author={Yige Hong and Qiaomin Xie and Yudong Chen and Weina Wang},
      year={2023},
      eprint={2306.00196},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      arxiv={https://arxiv.org/abs/2306.00196},
      %journal={arXiv preprint arXiv:2306.00196},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS), spotlight, },
      year={2023}
}

@article{pavse2023tackling,
      title={Tackling Unbounded State Spaces in Continuing Task Reinforcement Learning}, 
      author={Brahma S. Pavse and Yudong Chen and Qiaomin Xie and Josiah P. Hanna},
      year={2023},
      eprint={2306.01896},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      arxiv={https://arxiv.org/abs/2306.01896},
      journal={arXiv preprint arXiv:2306.01896}
}

@inproceedings{wu2023faking,
      title={On Faking a Nash Equilibrium}, 
      author={Young Wu and Jeremy McMahan and Xiaojin Zhu and Qiaomin Xie},
      year={2024},
      eprint={2306.08041},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      arxiv={https://arxiv.org/abs/2306.08041},
      %journal={arXiv preprint arXiv:2306.08041},
      booktitle={AAAI Conference on Artificial Intelligence}
}


@article{vlatakisgkaragkounis2023stochastic,
      title={Stochastic Methods in Variational Inequalities: Ergodicity, Bias and Refinements}, 
      author={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Angeliki Giannou and Yudong Chen and Qiaomin Xie},
      year={2023},
      eprint={2306.16502},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      arxiv={https://arxiv.org/abs/2306.16502},
      journal={arXiv preprint arXiv:2306.16502}
}

@inproceedings{huo2024effectiveness,
      title={Effectiveness of Constant Stepsize in Markovian LSA and Statistical Inference}, 
      author={Dongyan Huo and Yudong Chen and Qiaomin Xie},
      year={2024},
      booktitle={AAAI Conference on Artificial Intelligence},
      abstract={In this paper, we study the effectiveness of using a constant stepsize in statistical inference via linear stochastic approximation (LSA) algorithms with Markovian data. After establishing a Central Limit Theorem (CLT), we develop an inference procedure that uses the averaged LSA iterates to construct confidence intervals (CIs). Our procedure leverages the fast mixing property of constant-stepsize LSA for better covariance estimation, and employs Richardson-Romberg (RR) extrapolation to reduce the bias induced by a constant stepsize and Markovian data. We develop theoretical results for guiding stepsize selection in RR extrapolation, and identify several important settings where the bias provably vanishes even without extrapolation. We conduct extensive numerical experiments and compare against the classical diminishing stepsize regime. Our results show that using a constant stepsize enjoys easy hyperparameter tuning, fast convergence and consistently better CI coverage, especially with limited data.}
}

@inproceedings{chen2024exact_heavy_tailed,
      title={Exact Policy Recovery in Offline RL with Both Heavy-Tailed Rewards and Data Corruption}, 
      author={Yiding Chen and Xuezhou Zhang and Qiaomin Xie and Xiaojin Zhu},
      year={2024},
      booktitle={AAAI Conference on Artificial Intelligence},
      abstract={We study offline reinforcement learning (RL) with heavy-tailed reward distribution and data corruption: (i) Moving beyond subGaussian reward distribution, we require only a bounded $(1+\gamma)$-th moment for $\gamma \in (0,1]$; (ii) We allow corruptions where an attacker can arbitrarily modify $\epsilon$-fraction of the rewards and transitions in the dataset. We first derive a sufficient optimality condition for generalized Pessimistic Value Iteration (PEVI), which allows various estimators with proper confidence bounds and can be applied to multiple learning settings. In order to handle the data corruption and heavy-tailed reward setting, we prove that the trimmed-mean estimation achieves a minimax optimal error rate $O\big(\sigma \epsilon^{\frac{\gamma}{1+\gamma}}\big)$ for robust mean estimation under heavy-tailed distributions. In the PEVI algorithm, we plug in the trimmed mean estimation and the confidence bound to solve the robust offline RL problem. Standard analysis reveals that data corruption induces a bias term $O\big(H\sigma\epsilon^{\frac{\gamma}{1+\gamma}}+\epsilon H\big)$ in the suboptimality gap, which gives the false impression that any data corruption prevents optimal policy learning. By using the optimality condition for the generalized PEVI, we show that as long as the bias term is less than the ``action gap'', the policy returned by PEVI achieves the optimal value given sufficient data.}
}

@inproceedings{mcmahan2024attack_defense,
      title={Optimal Attack and Defense for Reinforcement Learning}, 
      author={Jeremy McMahan and Young Wu and Xiaojin Zhu and Qiaomin Xie},
      year={2024},
      booktitle={AAAI Conference on Artificial Intelligence},
      arxiv={https://arxiv.org/abs/2312.00198},
      eprint={2312.00198},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      %journal={arXiv preprint arXiv:2312.00198}
}